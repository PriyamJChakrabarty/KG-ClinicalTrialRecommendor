{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "URI = \"neo4j://127.0.0.1:7687\"\n",
    "AUTH = (\"neo4j\", \"12345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "ground_truth_df = pd.read_csv('ground_truth_recommendations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_trials_jaccard(driver, trial_id, top_n=10):\n",
    "    query = \"\"\"\n",
    "    MATCH (input:SubjectNode {name: $trial_id})\n",
    "    MATCH (input)-[:RELATIONSHIP]-(inputNeighbor:ObjectNode)\n",
    "    WITH input, COLLECT(DISTINCT inputNeighbor) AS inputNeighbors\n",
    "    \n",
    "    MATCH (other:SubjectNode)\n",
    "    WHERE other <> input\n",
    "    \n",
    "    MATCH (other)-[:RELATIONSHIP]-(otherNeighbor:ObjectNode)\n",
    "    WITH input, inputNeighbors, other, COLLECT(DISTINCT otherNeighbor) AS otherNeighbors\n",
    "    \n",
    "    WITH input, other,\n",
    "         inputNeighbors,\n",
    "         otherNeighbors,\n",
    "         [n IN inputNeighbors WHERE n IN otherNeighbors] AS intersection\n",
    "    WITH input, other,\n",
    "         SIZE(intersection) AS intersectionSize,\n",
    "         SIZE(inputNeighbors) + SIZE(otherNeighbors) - SIZE(intersection) AS unionSize\n",
    "    \n",
    "    WITH other.name AS similarTrial,\n",
    "         CASE WHEN unionSize = 0 THEN 0.0 \n",
    "              ELSE toFloat(intersectionSize) / toFloat(unionSize) \n",
    "         END AS similarity\n",
    "    \n",
    "    WHERE similarity > 0\n",
    "    RETURN similarTrial, similarity\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT $top_n\n",
    "    \"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, trial_id=trial_id, top_n=top_n)\n",
    "        return [(record[\"similarTrial\"], record[\"similarity\"]) for record in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations\n",
    "query_trials = ground_truth_df['query_trial'].unique()\n",
    "all_recommendations = {}\n",
    "\n",
    "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "for trial_id in query_trials:\n",
    "    similar_trials = find_similar_trials_jaccard(driver, trial_id, top_n=20)\n",
    "    all_recommendations[trial_id] = similar_trials\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MRR\n",
    "reciprocal_ranks = []\n",
    "\n",
    "for query_trial in all_recommendations.keys():\n",
    "    recommendations = [trial for trial, score in all_recommendations[query_trial]]\n",
    "    relevant_trials = ground_truth_df[ground_truth_df['query_trial'] == query_trial]['relevant_trial'].tolist()\n",
    "    \n",
    "    first_rank = None\n",
    "    for rank, recommended_trial in enumerate(recommendations, 1):\n",
    "        if recommended_trial in relevant_trials:\n",
    "            first_rank = rank\n",
    "            break\n",
    "    \n",
    "    if first_rank:\n",
    "        reciprocal_ranks.append(1.0 / first_rank)\n",
    "    else:\n",
    "        reciprocal_ranks.append(0.0)\n",
    "\n",
    "mrr = np.mean(reciprocal_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision@5 and Recall@5\n",
    "k = 5\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for query_trial in all_recommendations.keys():\n",
    "    recommendations = [trial for trial, score in all_recommendations[query_trial][:k]]\n",
    "    relevant_trials = ground_truth_df[ground_truth_df['query_trial'] == query_trial]['relevant_trial'].tolist()\n",
    "    \n",
    "    relevant_in_top_k = len([t for t in recommendations if t in relevant_trials])\n",
    "    total_relevant = len(relevant_trials)\n",
    "    \n",
    "    precision = relevant_in_top_k / k\n",
    "    recall = relevant_in_top_k / total_relevant if total_relevant > 0 else 0.0\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "precision_at_5 = np.mean(precisions)\n",
    "recall_at_5 = np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EVALUATION\")\n",
    "print(f\"MRR: {mrr:.4f}\")\n",
    "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
    "print(f\"Recall@5: {recall_at_5:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
